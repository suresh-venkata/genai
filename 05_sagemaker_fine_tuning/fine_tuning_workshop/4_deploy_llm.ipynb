{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy fine-tuned model\n",
    "Now that we have fine-tuned the model, we can deploy it to a SageMaker endpoint. There are numerous deployment [options](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) in SageMaker including RealTime, Serverless, Asynchronous, and Batch Transform. In this notebook, we will deploy the model as a RealTime endpoint. \n",
    "There are also numerous options for deploying LLMs for RealTime inference including:\n",
    "- Single model or multi-model endpoints\n",
    "- Instance Types (GPU, Inferentia2)\n",
    "- Various inference frameworks such as Large Model Inference, Text Generation Inference, TorchServe, and TensorRT LLM\n",
    "We'll use the Large Model Inference (LMI) container to deploy the LLM. \n",
    "\n",
    "Refer to the blog post [here](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-mixtral-and-llama-2-models-with-new-amazon-sagemaker-containers/) for detailed recommendations for configuring various model architectures for optimal performance on thr LMI container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "bucket = sess.default_bucket()  # default bucket name\n",
    "account_id = sess.account_id() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to upload our merged model to S3\n",
    "local_model_path = \"merged_model\"\n",
    "s3_model_path = f\"s3://{bucket}/banking-regulations-model\"\n",
    "!aws s3 sync {local_model_path} {s3_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inference environment for LLM\n",
    "# for more details see documentation here https://docs.djl.ai/master/docs/serving/serving/docs/lmi/deployment_guide/configurations.html\n",
    "llm_env = {\n",
    "    \"TENSOR_PARALLEL_DEGREE\": \"1\",  # use 1 GPUs\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\", # use VLLM rolling batch\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\", # max rolling batch size (controls the concurrency)\n",
    "    \"OPTION_DTYPE\": \"fp16\", # load weights in fp16\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"16384\", # max context length in tokens for the model\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\", # trust remote code\n",
    "    \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\", # use 95% of GPU memory\n",
    "}\n",
    "\n",
    "# create DJLModel object for LLM\n",
    "# see here for LMI version updates https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers \n",
    "sm_llm_model = DJLModel(\n",
    "    model_id=s3_model_path,\n",
    "    djl_version=\"0.30.0\",\n",
    "    djl_framework=\"djl-lmi\",\n",
    "    role=role,\n",
    "    env=llm_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(f\"bank-new\")\n",
    "\n",
    "predictor = sm_llm_model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             serializer=serializers.JSONSerializer(),\n",
    "             deserializer=deserializers.JSONDeserializer(),\n",
    "             container_startup_health_check_timeout=1800\n",
    "                                \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the endpoint with an example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "\n",
    "import json\n",
    "test_data = []\n",
    "with open(\"data/prepared_data/prepared_data_test.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "    \n",
    "\n",
    "inference_template = \"[INST] You are a Banking Regulations expert.\\nGiven this context\\nCONTEXT\\n{context}\\n Answer this question\\nQuestion: {question} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the endpoint with a sample question\n",
    "idx = 125\n",
    "context = test_data[idx][\"context\"]\n",
    "question = test_data[idx][\"question\"]\n",
    "answer = test_data[idx][\"answer\"]\n",
    "\n",
    "prompt = inference_template.format(context=context, question=question)\n",
    "\n",
    "response = predictor.predict(\n",
    "    {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":256, \"do_sample\":False, \"temperature\":0}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question: \", question)\n",
    "print(\"\\nGenerated Answer: \", response[\"generated_text\"])\n",
    "print(\"\\nGround Truth Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the endpoint name to a file so we can use it in the next notebook\n",
    "\n",
    "with open(\"endpoint_config.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"endpoint_name\": endpoint_name}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we deployed a fine-tuned model to a SageMaker endpoint using the Large Model Inference container. In the next notebook, we will incorporate the endpoint into our RAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
