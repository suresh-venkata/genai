{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Retrieval Augment Generation Solution with a Fine Tuned Model\n",
    "Now that we have a fine-tuned model, it's time to revisit our retrieval augment generation solution. In this notebook, we will re-implement the initial pipeline but this time using the fine-tuned model to generate the responses. We will then evaluate the performance of the fine-tuned model and compare it with the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    \"mistral.mistral-7b-instruct-v0:2\",\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from rich import print as rprint\n",
    "import json\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_aws.llms import SagemakerEndpoint\n",
    "from langchain_aws.chat_models import ChatBedrockConverse\n",
    "from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict\n",
    "\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "data_path = Path(\"data/prepared_data\")\n",
    "train_data = (data_path / \"prepared_data_train.jsonl\").read_text().splitlines()\n",
    "test_data = (data_path / \"prepared_data_test.jsonl\").read_text().splitlines()\n",
    "\n",
    "doc_ids = []\n",
    "documents = []\n",
    "\n",
    "# Create a list of LangChain documents that can then be used to ingest into a vector store\n",
    "\n",
    "for record in chain(train_data, test_data):\n",
    "    json_record = json.loads(record)\n",
    "    if json_record[\"ref_doc_id\"] not in doc_ids:\n",
    "        doc_ids.append(json_record[\"ref_doc_id\"])\n",
    "        doc = Document(page_content=json_record[\"context\"], metadata=json_record[\"section_metadata\"])\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\")\n",
    "bedrock_client = boto3_session.client(\"bedrock\")\n",
    "smr_client = boto3_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "embedding_modelId = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "embed_model = BedrockEmbeddings(\n",
    "    model_id=embedding_modelId,\n",
    "    model_kwargs={\"dimensions\": 1024, \"normalize\": True},\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "query = \"Do I really need to fine-tune the large language models?\"\n",
    "response = embed_model.embed_query(query)\n",
    "rprint(f\"Generated an embedding with {len(response)} dimensions. Sample of first 10 dimensions:\\n\", response[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can resuse the vector db from the initial model since we're keeping embeddings the same\n",
    "vector_store_file = \"baseline_rag_vec_db.pkl\"\n",
    "\n",
    "if not Path(vector_store_file).exists():\n",
    "    rprint(f\"Vector store file {vector_store_file} does not exist. Will create a new vector store.\")\n",
    "    CREATE_NEW = True\n",
    "else:\n",
    "    rprint(f\"Vector store file {vector_store_file} already exists. Delete it or change the file name above to create a new vector store.\")\n",
    "    CREATE_NEW = False \n",
    "\n",
    "if CREATE_NEW:\n",
    "    vec_db = FAISS.from_documents(documents, embed_model)\n",
    "    pickle.dump(vec_db.serialize_to_bytes(), open(vector_store_file, \"wb\"))\n",
    "    \n",
    "else:\n",
    "    if not Path(vector_store_file).exists():\n",
    "        raise FileNotFoundError(f\"Vector store file {vector_store_file} not found. Set CREATE_NEW to True to create a new vector store.\")\n",
    "    \n",
    "    vector_db_buff = BytesIO(pickle.load(open(vector_store_file, \"rb\")))\n",
    "    vec_db = FAISS.deserialize_from_bytes(serialized=vector_db_buff.read(), embeddings=embed_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "k = 3\n",
    "faiss_retriever = vec_db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "bm_25 = BM25Retriever.from_documents(documents)\n",
    "bm_25.k = k\n",
    "\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm_25], weights=[0.75, 0.25]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SageMaker endpoints with LangChain\n",
    "To use a SageMaker endpoint with LangChain we need to implement a `ContentHandler` class that will handle the preprocessing of the input data and the postprocessing of the output data. The primary reason for this is unlike Bedrock, a SageMaker API may have different and inconsistent input and output payload formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_endpoint_config_path = Path(\"endpoint_config.json\")\n",
    "if sagemaker_endpoint_config_path.exists():\n",
    "    endpoint_name = json.loads(sagemaker_endpoint_config_path.read_text())[\"endpoint_name\"]\n",
    "else:\n",
    "    rprint(f\"[bold red]Endpoint config file {sagemaker_endpoint_config_path} not found. Please make sure that you ran the prior training and deployment notebooks[/bold red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        \"\"\"formats the request payload for the model endpoint\"\"\"\n",
    "        \n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        \"\"\"extracts the generated answer from the endpoint response\"\"\"\n",
    "        \n",
    "        output = output.read().decode(\"utf-8\")\n",
    "        generated_answer = json.loads(output)[\"generated_text\"]\n",
    "        return generated_answer\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "tuned_llm = SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        client=smr_client,\n",
    "        model_kwargs={\"temperature\": 0, \"max_new_tokens\": 500},\n",
    "        content_handler=content_handler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the configured LLM we can reimplement the retrieval augment generation pipeline using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "tuned_prompt_template = \"[INST] You are a Banking Regulations expert.\\nGiven this context\\nCONTEXT\\n{context}\\n Answer this question\\nQuestion: {question} [/INST]\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(tuned_prompt_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "generate_tuned_answer = {\"answer\": prompt | tuned_llm | output_parser,\n",
    "                   \"context\": itemgetter(\"context\")}\n",
    "\n",
    "tuned_chain = setup_and_retrieval | generate_tuned_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample response \n",
    "\n",
    "sample_record = json.loads(test_data[150])\n",
    "sample_question = sample_record[\"question\"]\n",
    "sample_answer = sample_record[\"answer\"]\n",
    "rprint(f\"Sample question: {sample_question}\")\n",
    "response = tuned_chain.invoke(sample_question)\n",
    "generated_answer = response[\"answer\"]\n",
    "rprint(f\"\\nGenerated answer: {generated_answer}\")\n",
    "rprint(f\"\\nGround truth answer: {sample_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to speed up the inference and evaluation process\n",
    "\n",
    "async def generate_answer_async(rag_chain, example):\n",
    "    example = json.loads(example)\n",
    "    response = await rag_chain.ainvoke(example[\"question\"])\n",
    "    contexts = [doc.page_content for doc in response[\"context\"]]\n",
    "    row = {\"question\": example[\"question\"], \"answer\": response[\"answer\"], \"contexts\": contexts, \"ground_truth\": example[\"answer\"]}\n",
    "    return row\n",
    "\n",
    "async def evaluate_llm_async(metric, rows):\n",
    "    sem = asyncio.Semaphore(10)\n",
    "\n",
    "    async def limited_invoke(row):\n",
    "        async with sem:\n",
    "            return await metric.ainvoke(row)\n",
    "\n",
    "    tasks = [asyncio.create_task(limited_invoke(row)) for row in rows]\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE_LLM_EVALUATION = 100\n",
    "eval_rows = []\n",
    "for example in test_data[:NUM_SAMPLE_LLM_EVALUATION]:\n",
    "    eval_rows.append(generate_answer_async(tuned_chain, example))\n",
    "event_loop = asyncio.get_event_loop()\n",
    "eval_data= event_loop.run_until_complete(asyncio.gather(*eval_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the guardrail should be created as part of the workshop\n",
    "# if not you can create \"rag_eval\" guardrail in the console with only the contextual grounding check enabled\n",
    "eval_guardrail = [gr for gr in bedrock_client.list_guardrails()[\"guardrails\"] if gr[\"name\"]==\"rag_eval\"]\n",
    "if len(eval_guardrail) == 0:\n",
    "    rprint(\"No RAG evaluation guardrail found. Please create one in the Bedrock console.\")\n",
    "else:\n",
    "    eval_guardrail = eval_guardrail[0]\n",
    "eval_guardrail_id = eval_guardrail[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_rag_eval_guardrail(guardrail_id, question, context, response):\n",
    "\n",
    "    guardrail_payload = [\n",
    "        {\n",
    "            \"text\": {\n",
    "                \"text\": context,\n",
    "                \"qualifiers\": [\"grounding_source\"],\n",
    "            }\n",
    "        },\n",
    "        {\"text\": {\"text\": question, \"qualifiers\": [\"query\"]}},\n",
    "        {\"text\": {\"text\": response}},\n",
    "    ]\n",
    "\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=\"1\",\n",
    "        source=\"OUTPUT\",\n",
    "        content=guardrail_payload,\n",
    "    )\n",
    "    assessments = response[\"assessments\"][0][\"contextualGroundingPolicy\"][\"filters\"]\n",
    "    grounding_score = [\n",
    "        metric for metric in assessments if metric[\"type\"] == \"GROUNDING\"\n",
    "    ][0][\"score\"]\n",
    "    relevance_score = [\n",
    "        metric for metric in assessments if metric[\"type\"] == \"RELEVANCE\"\n",
    "    ][0][\"score\"]\n",
    "    return {\"grounding_score\": grounding_score, \"relevance_score\": relevance_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_guardrail(guardrail_id, questions, contexts, answers):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for question, context, answer in zip(questions, contexts, answers):\n",
    "            results.append(\n",
    "                executor.submit(\n",
    "                    invoke_rag_eval_guardrail, guardrail_id, question, \"\\n\".join(context), answer\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    eval_rows = [result.result() for result in results]\n",
    "        \n",
    "    grounding_scores = [row[\"grounding_score\"] for row in eval_rows]\n",
    "    relevance_scores = [row[\"relevance_score\"] for row in eval_rows]\n",
    "    grounding_score = sum(grounding_scores) / len(grounding_scores)\n",
    "    relevance_score = sum(relevance_scores) / len(relevance_scores)\n",
    "    return grounding_score, relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(\"Evaluating the fine-tuned RAG responses using the RAG evaluation guardrail\")\n",
    "ft_grounding_score, ft_relevance_score = evaluate_rag_guardrail(\n",
    "    eval_guardrail_id,\n",
    "    [row[\"question\"] for row in eval_data],\n",
    "    [row[\"contexts\"] for row in eval_data],\n",
    "    [row[\"answer\"] for row in eval_data],\n",
    ")\n",
    "\n",
    "rprint(f\"Fine-tuned grounding score: {ft_grounding_score}\\n\")\n",
    "rprint(f\"Fine-tuned relevance score: {ft_relevance_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_evals = {\n",
    "    \"grounding_score\": ft_grounding_score,\n",
    "    \"relevancy\": ft_relevance_score,\n",
    "\n",
    "}\n",
    "original_evals = json.loads(Path(\"base_evaluation.json\").read_text())\n",
    "\n",
    "ground_truth_grounding_score = original_evals[\"ground_truth\"][\"grounding_score\"]\n",
    "ground_truth_relevance_score = original_evals[\"ground_truth\"][\"relevancy\"]\n",
    "baseline_grounding_score = original_evals[\"baseline\"][\"grounding_score\"]\n",
    "baseline_relevance_score = original_evals[\"baseline\"][\"relevancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evals(grounding_scores, relevance_scores, names=[\"Fine-tuned\", \"Baseline\", \"Ground Truth\"]):\n",
    "    evals = pd.DataFrame(\n",
    "        {\n",
    "            \"Grounding Score\": grounding_scores,\n",
    "            \"Relevance Score\": relevance_scores,\n",
    "            \"Model\": names,\n",
    "        }\n",
    "    )\n",
    "    evals = evals.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "    ax = sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=evals)\n",
    "    plt.title(\"Grounding and Relevance Scores\")\n",
    "    \n",
    "    # Add value labels\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_height(), '.2f'),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center',\n",
    "                   xytext = (0, 9),\n",
    "                   textcoords = 'offset points')\n",
    "    \n",
    "    # Move legend to the bottom\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_evals(\n",
    "    [ft_grounding_score, baseline_grounding_score, ground_truth_grounding_score],\n",
    "    [ft_relevance_score, baseline_relevance_score, ground_truth_relevance_score],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "sagemaker_session = sagemaker.Session()\n",
    "pred = Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
