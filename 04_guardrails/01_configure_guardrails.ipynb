{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Guardrails configuration\n",
    "\n",
    "> *This notebook should work well with the **`Python 3 (ipykernel)`** kernel in SageMaker Studio on ml.t3.medium instance*\n",
    "\n",
    "In this lab, we will perform the following tasks to build and test a guardrail configuration.\n",
    "\n",
    "1. Define the name and the description\n",
    "2. Define the tone filter levels for different content types\n",
    "3. Define a denied topic to create an operational boundary \n",
    "4. Define a list of prohibited words that we don't want to allow in the user prompt and the LLM's response\n",
    "5. Define the personally identifiable information (PII) detection and masking\n",
    "6. Define a contextual grounding check policy\n",
    "7. Define the default responses if the guardrail blocks a request or a response\n",
    "8. Create a guardrail configuration using the Bedrock API\n",
    "9. Verify the guardrail configuration created in Bedrock\n",
    "\n",
    "We have a lot to cover. So, let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the environment\n",
    "\n",
    "First, we will import required libraries and validate the environment sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from rich import print as rprint\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create and test Bedrock client connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bedrock client for model access\n",
    "bedrock_client = boto3.client(\"bedrock\")\n",
    "\n",
    "# Validate bedrock client connection\n",
    "if bedrock_client is not None:\n",
    "    rprint(\"Successfully connected to Bedrock\")\n",
    "\n",
    "# Test Bedrock client connection by listing available guardrails in the account for current region\n",
    "try:\n",
    "    guardrails = bedrock_client.list_guardrails()\n",
    "    rprint(\"Successfully retrieved available guardrails.\")\n",
    "    rprint(guardrails)\n",
    "\n",
    "except Exception as e:\n",
    "    rprint(f\"Error creating guardrail: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the state of your account, you might not see listed guardrails in the response. However, a successful response validates the connection for the Bedrock client. \n",
    "\n",
    "### Define the name and the description\n",
    "\n",
    "Now, we will define various attributes and configuration items that we will use for our Bedrock Guardrails configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_int = int(time.time())\n",
    "\n",
    "# Define metadata\n",
    "guardrail_name = 'BasicGuardrail-' + str(epoch_int)\n",
    "guardrail_description = 'Basic guardrail configuration to be ' + \\\n",
    "                        'used with GenAI boot camp lab'\n",
    "rprint(f\"Guardrail Name: {guardrail_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tone filter levels\n",
    "\n",
    "Now, let's define a high-level tolerance boundary for hateful and insulting tones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tone filter strengths for hateful and insulting content\n",
    "\n",
    "guardrail_filters = {\n",
    "                'filtersConfig': [\n",
    "                    {\n",
    "                        'inputStrength': 'HIGH',\n",
    "                        'outputStrength': 'HIGH',\n",
    "                        'type': 'HATE',\n",
    "                        'inputModalities': ['TEXT', 'IMAGE'],\n",
    "                        'outputModalities': ['TEXT', 'IMAGE']\n",
    "                    },\n",
    "                    {\n",
    "                        'inputStrength': 'MEDIUM',\n",
    "                        'outputStrength': 'MEDIUM',\n",
    "                        'type': 'INSULTS',\n",
    "                        'inputModalities': ['TEXT', 'IMAGE'],\n",
    "                        'outputModalities': ['TEXT', 'IMAGE']\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'VIOLENCE',\n",
    "                        'inputStrength': 'HIGH',\n",
    "                        'outputStrength': 'HIGH',\n",
    "                        'inputModalities': ['TEXT', 'IMAGE'],\n",
    "                        'outputModalities': ['TEXT', 'IMAGE']\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'PROMPT_ATTACK',\n",
    "                        'inputStrength': 'HIGH',\n",
    "                        'outputStrength': 'NONE',\n",
    "                        'inputModalities': ['TEXT'],\n",
    "                        'outputModalities': ['TEXT']\n",
    "                    }\n",
    "                ]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous configuration, we have just configured the filters for hateful and insulting content. However, Bedrock Guardrails also supports other tones including misconduct, sexuality, and violence. \n",
    "\n",
    "### Define a denied topic\n",
    "In the following configuration, we will define certain topics that we don't want to entertain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a denied topic to decline any asks for a financial advice with a couple of examples\n",
    "\n",
    "guardrail_deny_topics = {\n",
    "                'topicsConfig': [\n",
    "                    {\n",
    "                        'name': 'Financial Advice',\n",
    "                        'definition': 'Providing financial or investment advice',\n",
    "                        'examples': [\n",
    "                            'What stocks should I invest in?',\n",
    "                            'How should I invest my money?'\n",
    "                        ],\n",
    "                        'type': 'DENY'\n",
    "                    }\n",
    "                ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the previous configuration includes only one deny topic, you may add more topics in the list. As of this writing, you can only configure the `DENY` topics.\n",
    "\n",
    "### Define a list of prohibited words\n",
    "Now, let's configure a word exclusion policy to list certain words that we want to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a word exclusion policy\n",
    "\n",
    "guardrail_deny_words = {\n",
    "                'wordsConfig': [\n",
    "                    {\n",
    "                        'text': 'idiot'\n",
    "                    },\n",
    "                    {\n",
    "                        'text': 'gun'\n",
    "                    }\n",
    "                ],\n",
    "                'managedWordListsConfig': [\n",
    "                    {\n",
    "                        'type': 'PROFANITY'\n",
    "                    }\n",
    "                ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we listed two specific words that we want to deny - `idiot` and `gun`. Other than such specific words that you want to deny, Bedrock maintains a list of words in a specific area - profanity. As of this writing, this is the only supported list of words offered by Bedrock. \n",
    "\n",
    "### Define a PII handling policy\n",
    "Now, let's configure information redaction policy to mask certain sensitive financial PII details like a credit card number, a credit card CVV, and a credit card expiry date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensitive information policy\n",
    "\n",
    "guardrail_redact_data = {\n",
    "                'piiEntitiesConfig': [\n",
    "                    {\n",
    "                        'type': 'CREDIT_DEBIT_CARD_NUMBER',\n",
    "                        'action': 'ANONYMIZE'\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'CREDIT_DEBIT_CARD_CVV',\n",
    "                        'action': 'BLOCK'\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'CREDIT_DEBIT_CARD_EXPIRY',\n",
    "                        'action': 'BLOCK'\n",
    "                    }\n",
    "                ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous configuration, we only configured three PII data elements. However, Bedrock Guardrails supports several such data types, including but not limited to names, addresses, zipcode, phone number, email address, etc. For a full list of configuration please check [this website](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GuardrailPiiEntityConfig.html). Additionally, you can also define your own data elements for this purpose using their regex patterns as described in [this website](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GuardrailRegexConfig.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a contextual grounding check policy\n",
    "\n",
    "Now, let's define a contextual grounding check policy that will check model's responses to see if they are faithful to give context data and relevant to the request sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contextual grounding check policy\n",
    "\n",
    "contextual_grounding_policy = {\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'GROUNDING',\n",
    "                'threshold': 0.7\n",
    "            },\n",
    "            {\n",
    "                'type': 'RELEVANCE',\n",
    "                'threshold': 0.7\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the default responses for the blocked content\n",
    "Now, let's define how to handle all these policy exceptions when we encounter any denied content in the request and the response generated by an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what should the users get when their \n",
    "# request is blocked by our guardrail configuration.\n",
    "guardrail_blocked_input_message = (\n",
    "                    'Sorry, but I cannot help you with this topic '\n",
    "                    'as it violates my responsible behavior policy.'\n",
    ")\n",
    "\n",
    "# Define what should the users get when the LLM generates\n",
    "# some content that violates our guardrail configuration.\n",
    "guardrail_blocked_output_message = (\n",
    "                    'Sorry, but I cannot generate a response to your '\n",
    "                    'ask that does not violate my responsible behavior policy.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a guardrail configuration in Bedrock\n",
    "Now, as we have all required configurations to create our solid guardrail policy, let's create one using the Bedrock client we created in the beginning of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a Bedrock Guardrail configuration\n",
    "guardrail_id = None\n",
    "guardrail_arn = None\n",
    "guardrail_version = None\n",
    "try:\n",
    "    guardrail_response = bedrock_client.create_guardrail(\n",
    "        name=guardrail_name,\n",
    "        description=guardrail_description,\n",
    "        contentPolicyConfig=guardrail_filters,\n",
    "        topicPolicyConfig=guardrail_deny_topics,\n",
    "        wordPolicyConfig=guardrail_deny_words,\n",
    "        sensitiveInformationPolicyConfig=guardrail_redact_data,\n",
    "        blockedInputMessaging=guardrail_blocked_input_message,\n",
    "        blockedOutputsMessaging=guardrail_blocked_output_message,\n",
    "        contextualGroundingPolicyConfig=contextual_grounding_policy\n",
    "    )\n",
    "    guardrail_id = guardrail_response['guardrailId']\n",
    "    guardrail_arn = guardrail_response['guardrailArn']\n",
    "    # Create a version of the guardrail\n",
    "    version_response = bedrock_client.create_guardrail_version(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        description='Initial version'\n",
    "    )\n",
    "    guardrail_version = version_response['version']\n",
    "except Exception as e:\n",
    "    rprint(f\"Error creating guardrail: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "rprint(f\"Guardrail created successfully with ID \\\"{guardrail_id}\\\" and version \\\"{guardrail_version}\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the guardrail identifiers in a file, so that we can use them in the next labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save guardrail details in first notebook\n",
    "Path(\"guardrail_config.json\").write_text(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"guardrail_id\": guardrail_id,\n",
    "            \"guardrail_arn\": guardrail_arn,\n",
    "            \"guardrail_version\": guardrail_version\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Please note the guardrail ID from the last line of the output. We will use this ID to pull this configuration for validations in the upcoming labs.\n",
    "```\n",
    "\n",
    "### Verify the guardrail configuration created in Bedrock\n",
    "Now, as we created the guardrail successfully, let's verify the configuration of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Bedrock Guardrail configuration with its ID and version\n",
    "guardrail = None\n",
    "try:\n",
    "    # Get the guardrail configuration\n",
    "    guardrail = bedrock_client.get_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version\n",
    "    )\n",
    "\n",
    "    # Describe details of the guardrail configuration\n",
    "    if guardrail is not None:\n",
    "        # Print guardrail details\n",
    "        rprint(\"\\nGuardrail Configuration:\")\n",
    "        rprint(f\"Name: {guardrail['name']}\")\n",
    "        rprint(f\"Description: {guardrail['description']}\")\n",
    "\n",
    "        # Print policy configurations if they exist\n",
    "        rprint(\"===================================================\")\n",
    "        if 'contentPolicy' in guardrail:\n",
    "            rprint(\"\\nContent Policy Configuration:\")\n",
    "            rprint(json.dumps(guardrail['contentPolicy'], indent=2))\n",
    "\n",
    "        rprint(\"===================================================\")\n",
    "        if 'topicPolicy' in guardrail:\n",
    "            rprint(\"\\nTopic Policy Configuration:\")\n",
    "            rprint(json.dumps(guardrail['topicPolicy'], indent=2))\n",
    "\n",
    "        rprint(\"===================================================\")\n",
    "        if 'wordPolicy' in guardrail:\n",
    "            rprint(\"\\nWord Policy Configuration:\")\n",
    "            rprint(json.dumps(guardrail['wordPolicy'], indent=2))\n",
    "\n",
    "        rprint(\"===================================================\")\n",
    "        if 'sensitiveInformationPolicy' in guardrail:\n",
    "            rprint(\"\\nSensitive Information Policy Configuration:\")\n",
    "            rprint(json.dumps(guardrail['sensitiveInformationPolicy'],\n",
    "                              indent=2))\n",
    "        rprint(\"===================================================\")\n",
    "        if 'contextualGroundingPolicy' in guardrail:\n",
    "            rprint(\"\\nContextual Grounding Policy Configuration:\")\n",
    "            rprint(json.dumps(guardrail['contextualGroundingPolicy'],\n",
    "                              indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    rprint(f\"Error getting guardrail configuration: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "In this lab, you learned how to create a Bedrock Guardrails configuration with different policies. In the next lab, we will test this guardrail configuration to validate a user prompt independent of a large language model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
