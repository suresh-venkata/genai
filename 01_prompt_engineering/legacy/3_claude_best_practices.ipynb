{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started with using Anthropic models using Amazon Bedrock\n",
    "\n",
    "> *This notebook should work well with the **`conda_python3`** kernel in SageMaker Studio on ml.t3.medium instance*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the boto3 Python SDK to work with Amazon Bedrock Foundation Models. If you are running this in AWS provided accounts, excessive API calls to Bedrock APIs may results in throttling and your account may get blocked\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access Claude, the latest version of Anthropic’s large language model (LLM), on Amazon Bedrock. Claude can take up to 200,000 tokens in each prompt, meaning it can work over hundreds of pages of text, or even an entire book. Claude 2 can also write longer documents—on the order of a few thousand tokens—compared to its prior version, giving you even greater ways to develop generative AI applications using Amazon Bedrock.\n",
    "\n",
    "Anthropic, an AI safety and research lab that builds reliable, interpretable, and steerable AI systems, is the maker of the state-of-the art LLM, Claude. The new version of the LLM, Claude 3, can process large amounts of text. With its 200,000 token context window, which is equivalent to about 200 pages of information, Claude lets you process large amounts of data in a single prompt. As a result, you can use documents, emails, FAQs, chat transcripts or even entire codebases as inputs for Claude to edit, rewrite, summarize, answer questions, generate code, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v1\",\n",
    "    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Bedrock Client\n",
    "\n",
    "To help with this, we've provided a get_bedrock_client() utility method that supports passing in different options. You can find the implementation in ../utils/bedrock.py\n",
    "\n",
    "The get_bedrock_client() method accepts runtime (default=True) parameter to return either bedrock or bedrock-runtime client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "\n",
    "from utils.prompt_utils import prompts_to_messages, convert_pdf_to_image, convert_pil_image_to_b64\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Great Gatsby\n",
    "We will explore the large context capabilities of Claude by using the entire text of the book \"The Great Gatsby\" by F. Scott Fitzgerald as input. The Great Gatsby is a novel by American writer F. Scott Fitzgerald. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"./data/the-great-gatsby.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "text = ''.join([page.extract_text() for page in reader.pages])\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "total_no_of_words_input = len(text.split())\n",
    "# total_no_of_tokens = total_no_of_words/0.75\n",
    "print(f\"Total Number of Pages in Great Gatsby Book: {number_of_pages}\")\n",
    "print(f\"Total Number of words in Great Gatsby Book: {total_no_of_words_input}\")\n",
    "# print(f\"Total Number of tokens in Great Gatsby Book: {total_no_of_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get create a prompt and invoke Claude 3.5 Haiku\n",
    "In this example we will provide the first two chapters of the book to Claude and ask it to generate a list and description of each of the main characters in the book. As this is a widely known novel, Claude should be able to provide a good summary of the characters without any external context. However, this is not what we want in this case as we want to have Claude use the context provided to generate the output. We will therefore include a specific instruction to Claude to use the context provided in the prompt.\n",
    "\n",
    "Try experimenting yourself to see what happens if you remove the instruction to only use the context provided in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "\n",
    "two_chapters = text[:text.index(\"Chapter 3\")]\n",
    "\n",
    "prompt = f\"\"\"Here is The Great Gatsby Book: \n",
    "<book>\n",
    "{two_chapters}\n",
    "</book>\n",
    "\n",
    "I would like to know the list all the characters from this book. Describe the main characters and their roles in this book.\n",
    "Only use the text provided above to generate the list of characters, do not use any external sources or your own knowledge.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": prompts_to_messages(prompt),\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "modelId = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"  # change this to use a different version from the model provider\n",
    "# modelId = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "response_completion = response_body.get(\"content\")[0][\"text\"]\n",
    "rprint(response_completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Demand Pricing calcualtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer https://aws.amazon.com/bedrock/pricing/ for latest on-demand provisioned throughput pricing details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "total_no_words_output = len(response_completion.split())\n",
    " \n",
    "total_no_of_tokens_input = response['ResponseMetadata']['HTTPHeaders']['x-amzn-bedrock-input-token-count']\n",
    "total_no_tokens_output = response['ResponseMetadata']['HTTPHeaders']['x-amzn-bedrock-output-token-count']\n",
    "total_response_time = response['ResponseMetadata']['HTTPHeaders']['x-amzn-bedrock-invocation-latency']\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total Number of words in Great Gatsby Book: {total_no_of_words_input}\")\n",
    "print(f\"Total Number of tokens in Great Gatsby Book: {total_no_of_tokens_input}\")\n",
    "print(f\"Total Number of words in Output: {total_no_words_output}\")\n",
    "print(f\"Total Number of tokens in Output: {total_no_tokens_output}\")\n",
    "\n",
    "print('Execution time:', int(total_response_time)/1000, 'seconds')\n",
    "\n",
    "cost_of_input_tokens = int(total_no_of_tokens_input) * 0.0008 / 1000 #on-demand pricing per input token for Claude 3.5 Haiku\n",
    "cost_of_output_tokens = int(total_no_tokens_output) * 0.004 /100 #on-demand pricing per output token for Claude 3.5 Haiku\n",
    "total_cost = cost_of_input_tokens + cost_of_output_tokens\n",
    "\n",
    "print(f\"Total cost for input tokens: {cost_of_input_tokens:.6f}\")\n",
    "print(f\"Total cost of output tokens: {cost_of_output_tokens:.6f}\")\n",
    "print(f\"Total cost: ${total_cost:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load Amazon's annual report and analyze the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "amazon_reader = PdfReader(\"./data/Amazon-2022-Annual-Report.pdf\")\n",
    "amazon_number_of_pages = len(amazon_reader.pages)\n",
    "amazon_text = ''.join([page.extract_text() for page in amazon_reader.pages])\n",
    "print(amazon_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up prompt template with following best practices\n",
    "\n",
    "Anthropic provides a number of best practices for creating prompts that can help you get the best results from Claude.  These include:\n",
    "- Be clear and direct\n",
    "- Provide examples (i.e. multi-shot learning)\n",
    "- Let Claude think (Chain of Thought prompting)\n",
    "- Use XML tags\n",
    "- Provide a system prompt\n",
    "- Prefill Claude's response\n",
    "\n",
    "We will explore some of these best practices in this notebook but always refer to the [Anthropic documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview) for the most up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = f\"\"\"Here is Amazon financial annual report: \n",
    "<report>\n",
    "{amazon_text}\n",
    "</report>\n",
    "\n",
    "You are a helpful financial analyst. Please do the following:\n",
    "1. Summarize the annual report and provide highlights in bullet points. Place output into <summary></summary> tags.)\n",
    "2. What is the total value creation in 2022?\n",
    "3. What is the total value creation in 2020?\n",
    "4. How was year 2020 different from 2022 for the shareholders?\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Invoke Claude model using Bedrock API\n",
    "\n",
    "The Anthropic Claude models support the following parameters to control the length of the generated response.\n",
    "\n",
    "Maximum length (max_tokens_to_sample) – Specify the maximum number of tokens to use in the generated response.\n",
    "\n",
    "Stop sequences (stop_sequences) – Configure up to four sequences that the model recognizes. After a stop sequence, the model stops generating further tokens. The returned text doesn't contain the stop sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": prompts_to_messages(prompt_data),\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "modelId = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"  # change this to use a different version from the model provider\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body.get(\"content\")[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prompt guidance for different NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction\n",
    "A common use case for LLMs is to extract information from a document. To do this, we often want the model to provide a consistent output in a specified json format. We can accomplish this with Claude by feeding it our target schema as part of the prompt. Rather than having to manually craft the json schema, it is often better to utilize the [pydantic](https://docs.pydantic.dev/latest/) library to generate the schema for us from a data model that we design with Python classes.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# first we define a data model for the executive officer containing the information we want to extract\n",
    "# In this case we want to extract the name, age, position and description of the executive officer\n",
    "class ExecutiveOfficer(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the executive officer\")\n",
    "    age: str = Field(..., description=\"Age of the executive officer\")\n",
    "    position: str = Field(..., description=\"Position held by the executive officer\")\n",
    "    description: str = Field(..., description=\"Description about the executive officer\")\n",
    "\n",
    "# since there are multiple executive officers, we define a data model for the list of executive officers\n",
    "class ExecutiveOfficers(BaseModel):\n",
    "    executive_officers: List[ExecutiveOfficer] = Field(..., description=\"A list of executive officers\")\n",
    "    \n",
    "\n",
    "# next we generate a schema that we can feed to the prompt\n",
    "schema = {k: v for k, v in ExecutiveOfficers.schema().items()}\n",
    "\n",
    "\n",
    "# Finally we create the prompt where we explain how to interpret the schema and what information to extract\n",
    "information_extraction_template = f\"\"\"Here is Amazon financial annual report: \n",
    "<report>\n",
    "{amazon_text}\n",
    "</report>\n",
    "\n",
    " Please precisely extract information about executive officers.\n",
    "\n",
    "1. Name of the executive officer\n",
    "2. Age\n",
    "3. Position held\n",
    "4. Description about each of the officer\n",
    "\n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\n",
    "the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n",
    "\n",
    "Here is the output schema:\n",
    "<schema>\n",
    "{schema}\n",
    "</schema>\n",
    "\n",
    "Place the output in <output></output> tags.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": information_extraction_template}],\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "\n",
    "modelId = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"  # change this to use a different version from the model provider\n",
    " \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "# since we instructed the model to output the JSON instance in the <output></output> tags, we can easily extract it with a regular expression\n",
    "json_output = re.findall(r'<output>(.*?)</output>', response_body.get(\"content\")[0][\"text\"], re.DOTALL)[0]\n",
    "\n",
    "# we can also validate the output against the schema. In more complex LLM chains, we can use the output of a failed validation to have the model correct the output\n",
    "try:\n",
    "    ExecutiveOfficers.parse_raw(json_output)\n",
    "    print(f\"Output is valid JSON and conforms to the schema\")\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing output: {e}\")\n",
    "    \n",
    "rprint(Markdown(f'```json\\n{json_output}```'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction from Images\n",
    "Information extraction is not limited only to text based data. We can also extract information from images using the same approach. Let's revisit the JPMC annual report example from the prior notebook but this time we'll use pydantic to generate a more robust schema for the extracted information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# First we read the PDF file\n",
    "# Then we convert the PDF to an image\n",
    "\n",
    "pdf_path = \"data/jpmc_annual_report_page_6.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "img = convert_pdf_to_image(doc, page_number=0, dpi=150)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to define the data points for each year. This will be used to define the data points for all of the annual metrics\n",
    "class DataPoint(BaseModel):\n",
    "    year: str = Field(..., description=\"Year of the data point\")\n",
    "    value: float = Field(..., description=\"Value of the data point\")\n",
    "\n",
    "# class to define the annual metrics including the units and data points\n",
    "class AnnualMetrics(BaseModel):\n",
    "    units: str = Field(..., description=\"Units of the data such as $B, %, etc.\")\n",
    "    data_points: List[DataPoint] = Field(..., description=\"List of data points\")\n",
    "\n",
    "# finally we define the financial metrics that include the net income, diluted EPS, and ROTCE\n",
    "class FinancialMetrics(BaseModel):\n",
    "    net_income: AnnualMetrics = Field(..., description=\"Net income data\")\n",
    "    diluted_eps: AnnualMetrics = Field(..., description=\"Diluted earnings per share (EPS) data\")\n",
    "    rotce: AnnualMetrics = Field(..., description=\"Return on average tangible common equity (ROTCE) data\")\n",
    "    \n",
    "\n",
    "# next we generate a schema that we can feed to the prompt\n",
    "schema = {k: v for k, v in FinancialMetrics.schema().items()}\n",
    "\n",
    "text_prompt = f\"\"\"\n",
    "Extract the data contained in the bar graph and provide it in a json format. The data should include the Net Income, Diluted EPS, and ROTCE from 2004 to 2022.\n",
    "\n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\n",
    "the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n",
    "\n",
    "Here is the target output schema:\n",
    "{schema}\n",
    "Do not include the schema in the output, just generate the JSON instance.\n",
    "\n",
    "Place the output in <output></output> tags.\n",
    "\n",
    "\"\"\"\n",
    "image_prompt = convert_pil_image_to_b64(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"max_tokens\": 2048,\n",
    "    \"messages\": prompts_to_messages([{\"role\": \"user\", \"text_prompt\": text_prompt, \"image_prompt\": image_prompt}]),\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "modelId = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"  \n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "json_output = re.findall(r'<output>(.*?)</output>', response_body.get(\"content\")[0][\"text\"], re.DOTALL)[0]\n",
    "\n",
    "try:\n",
    "    FinancialMetrics.parse_raw(json_output)\n",
    "    print(f\"Output is valid JSON and conforms to the schema\")\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing output: {e}\")\n",
    "    \n",
    "rprint(Markdown(f'```json\\n{json_output}```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the model generate the expected output? If not, what went wrong? What could you do to improve the output? \n",
    "It is important to understand that the model is not perfect and may not always generate the expected output. Therefore, you must always perform validation checks on the output to ensure that it meets your requirements. If the output is not as expected, you can try the following steps to improve the output:\n",
    "- Use a larger model\n",
    "- Add an automated validation step to have the model validate itself\n",
    "- Perform additional prompt engineering to guide the model to generate the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PII Redaction\n",
    "When working with sensitive data, it is important to ensure that any personally identifiable information (PII) is redacted from the output. Claude can be used to redact PII from a document by providing a list of PII entities to redact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pii_extraction_template = f\"\"\"Here is some text. We want to remove all personally identifying information from this text and replace it with ****.  It's very important that names, phone numbers, and email addresses, gets replaced with ******.\n",
    "Here is the text, inside <text></text> XML tags\n",
    "\n",
    "<text>\n",
    "Phone Directory:\n",
    "John Latrabe, 800-232-1995,  john909709@geemail.com\n",
    "Josie Lana, 800-759-2905,   josie@josielananier.com\n",
    "Keven Stevens, 800-980-7000,  drkevin22@geemail.com\n",
    "\n",
    "Phone directory will be kept up to date by the HR manager.\n",
    "</text>\n",
    "\n",
    "Please put your sanitized version of the text with PII removed in <response></response> XML tags\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": pii_extraction_template}],\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "modelId = \"anthropic.claude-3-haiku-20240307-v1:0\"  # change this to use a different version from the model provider\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body.get(\"content\")[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Encoding Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges with reading PDF docs\n",
    "\n",
    "Certain PDFs can not simply be read using packages like PyPDF. PDF files don't have a character encoding like text files do. They are binary files and contain many different elements, including text, images, metadata, and more. The text within a PDF can be stored in several different ways, including as plain text, as a part of the PDF's internal structure, or as a part of an embedded font. You may have to perform additional pre-processing steps such as OCR before feeding the data to the model. Below is an example where PDF extraction doesn't yield the desired text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "one_up_reader = PdfReader(\"./data/one-up-on-wall-street-full.pdf\")\n",
    "one_up_number_of_pages = len(one_up_reader.pages)\n",
    "one_up_text = '\\n'.join([page.extract_text() for page in one_up_reader.pages])\n",
    "#one_up_text = one_up_text[:50000]\n",
    "print(one_up_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "one_up_data = f\"\"\"Here is book about Mr. Peter Lynch: \n",
    "<book>\n",
    "{one_up_text}\n",
    "</book>\n",
    "\n",
    "You are a financial analyst. Please answer the question from the book only if you can find relevant context is present, if you dont have the context from the book, Say I dont know:\n",
    "According to the author of the above book, what is the most important factor driving long-term value of a stock?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": one_up_data}],\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "modelId = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"  # change this to use a different version from the model provider\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body.get(\"content\")[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you got familiar with\n",
    "1. Working with Claude models \n",
    "2. How to leverage Claude's long context length and usecases where this might be a best fit\n",
    "3. Best practices and recommendations while using Claude models\n",
    "4. Leveraging Claude in different NLP tasks using promptig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
